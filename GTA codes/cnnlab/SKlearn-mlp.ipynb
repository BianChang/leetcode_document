{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKlearn - neural networks\n",
    "\n",
    "Import the digits data set and train a multilayer perceptron (MLP). The digits dataset, which is a simplified version of the very famous MNIST data set. This is a set of images, in black and white, of hand-drawn numbers from 0 to 9. Before proceeding, we will look at some of the data.\n",
    "\n",
    "As before, we will be using the sklearn library, which is adequate for small multilayer perceptrons. SKlearn is not intended to be a deep-learning library, and does not support parallelisation via GPUs. This means it is fine for our small examples (<1000 rows, <50 features, 2-3 hidden layers), but becomes very slow for anything larger.\n",
    "\n",
    "As you go through this notebook, recall from previous weeks the very particular way that sklearn initializes, trains, runs, and evaluates a model. We've provided some example code to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a dataset -  we will use one of the scikit built in datasets again\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one image\n",
    "image1 = digits.images[1]\n",
    "plt.imshow(image1, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should understand what the cmap parameter is doing here. If you do not, read the documentation for imshow (via a google search). The data in *images* is stored as a 2-d array, so that the rows and columns of the array match up with the physical coordinates of the image. The data in *data* is stored as a 1-d array, as shown below. You should compare both arrays to ensure you understand how the image data has been converted. From here, we will be working with data, as the sklearn functions take a 1-D array as input.\n",
    "\n",
    "Note that this means, for the simple MLP we will be training here, it does not matter which order the image pixels are placed in the input vector. That's not the case for a CNN where the convolution layer is applied to grids of nearby pixels. Randomly shuffling the pixels in the image prior to training a CNN would harm performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.images[1])\n",
    "print(digits.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the MLP. As part of good practice, we scale the digits data. The effect of this is to ensure that any resulting gradients are of a similar scale. This will make the gradient descent process more likely to converge quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled = preprocessing.scale(digits.data)\n",
    "\n",
    "# split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, digits.target, test_size = 0.3)\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(20, 20), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above score is providing the accuracy - think about whether this is a good metric for this classifier.\n",
    "\n",
    "Has the model finished training? (and if not, what should you change?)\n",
    "\n",
    "Using the code above as a template, experiment with changing some of the free parameters. What happens when you vary the number of hidden layers, neurons, and activation function? (refer to the sklearn documentation if you do not know how to do this). Create appropriate plots to show your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = clf.loss_curve_\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
