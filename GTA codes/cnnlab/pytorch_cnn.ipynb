{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f70545-b995-4d95-885e-c562c1643f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training a CNN with Pytorch\n",
    "We will now be using a expanded version of the digits data set to train a convolutional neural network.\n",
    "\n",
    "We will be using the Pytorch library, which is one of the main deep learning libraries. In comparison to SKlearn, Pytorch provides much more control over the architecture of your neural network, allowing you to create your own types of layers.\n",
    "\n",
    "## Import libraries, including PyTorch\n",
    "If you are using your own computer, you will need to install Pytorch. Setup guidance is here: https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b609b7-869f-4ebd-ad08-8e961b0fe73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "torch.set_num_threads(2)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a012c-638d-4ec0-8771-4efb2a6bdad8",
   "metadata": {},
   "source": [
    "The next lines check if your computer has CUDA GPU capabilities and uses it if it can. Elab doesn't have available GPUs, this means that the model will still train, but it will be slower. More explanation below on what speeds you should be expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9d008-6f39-4e37-8974-e64cb5ea1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c77cbd-c7f6-44de-b385-25c00831d5aa",
   "metadata": {},
   "source": [
    "## Import some data\n",
    "PyTorch has some built in datasets, which we will be using. We will be using the MNIST digits data set, which is a larger version of the sklearn digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4d6df-97cd-407b-827c-7e951b9c49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    "    )\n",
    "    \n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2a0ca-115c-456b-ac5d-74388faaef60",
   "metadata": {},
   "source": [
    "## View basic data set information\n",
    "Now that the dataset has been imported, you can view information about the train/test set using print(train_data) if you wish. You will see that there are 60000 training examples and 10000 test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab61409-5bec-4d34-8639-c1e434d31d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a43446-f966-477a-843e-234facef57fc",
   "metadata": {},
   "source": [
    "Like the Sklearn task, let's plot one of the images to see what we're looking at. Note that these are grayscale images (i.e. different shades of grey) rather than full RGB colour images. The assignment data set will contain colour images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b0a09-dab4-4e36-bbd0-046e5fc18abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data.data[1], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data[1].size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fe17f-d5ec-47b4-af1d-e792e36f3082",
   "metadata": {},
   "source": [
    "# Create the neural network architecture as a new class, CNN\n",
    "\n",
    "Defining a 'class' is beyond the scope of the course here - for a thorough introduction, read some material on Object Oriented Programming. Briefly, we can think of a 'class' as a grouping for a set of related variables and functions.\n",
    "\n",
    "The CNN class below creates the neural network architecture. Note that nn.sequential means that layers are added sequentially, so conv1 consists of a conv2d layer, a ReLU activation and a max pooling layer. Each layer type has its own parameters (in_channels, out_channels, etc.)\n",
    "\n",
    "In conv1, the convolution (conv2d) layer, we create 16 convolution filters (out_channels), a free choice by us (it can be any number). \n",
    "\n",
    "In the 2nd set of convolutions, defined by *conv2*, we have 16 convolutions from *conv1*, so our first parameter is 16 and we choose to have 32 output filters. Note that, for *conv2*, the parameters have been written in shorthand, without explicitly referring to the parameter variable name.\n",
    "\n",
    "The function, forward, describes how the whole network is put together from its component layers. It shows how we first put our input through *conv1*, then *conv2*, then *out* (a fully connected layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3117cd-2256-4859-9b4b-2cab285f5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(32, 64, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        #self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "        self.out = nn.Linear(64 * 3 * 3, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180fc06-a45d-4777-ba0c-141e2b9f63f8",
   "metadata": {},
   "source": [
    "## Instantiate the neural network object\n",
    "The class code above describes what our CNN object looks like, but we need to actual create an instance of the object. We do a similar thing when we train models in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf63a0e-221d-472d-afea-e6aa0b11facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN() # note that the variable name can be anything here, but I've chosen cnn for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50797625-2306-4761-b571-f44c02deec45",
   "metadata": {},
   "source": [
    "# Define the loss function and optimizer \n",
    "We now define the loss function and optimizer to train the network. Unlike standard stochastic gradient descent (what we looked at in the lectures), Adam is a variant that uses an adaptive learning rate to allow for faster convergence in many situations.\n",
    "\n",
    "We set the initial learning rate at 0.01. This is quite large and may lead to non-convergence in other problems, but in this case I know it works (through trial and error), and it means that we will be able to see the impact of training in relatively few iterations\n",
    "\n",
    "We use the cross entropy loss, which is the go-to for classification problems, rather than something like the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a38e1a-7ede-458b-b4fc-974bc5ce87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()   \n",
    "from torch import optim\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839952f-df64-438b-b5b6-9a0c2bb96528",
   "metadata": {},
   "source": [
    "# Set up training parameters\n",
    "\n",
    "Before attempting to train, we use the dataloader utility in pytorch to set up how training will work: \n",
    "the *batch_size* sets the mini-batch used in SGD (i.e. how many examples are considered at each step - in the lectures, we consider one example at a time, so-called online SGD). *shuffle* shuffles the data at each epoch, to reduce the likelihood of a weird convergence. This might happen, for instance, if all of the digits are in order, so that all of the 1s are trained first. This could lead to a situation where the network thinks that it has finished training because it sees thousands of examples of 1s, and only trains to recognise 1s. *num_workers* relates to how the data is handled in the memory (e.g. whether there are multiple sub-processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad48999-e60b-435b-8f82-efee7ad04376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1bceb8-402c-4b30-82e2-f7ac0b613fb6",
   "metadata": {},
   "source": [
    "## Training function\n",
    "We now define how model training works, over num_epochs. Note how the loss is computed. Gradients are computed using loss.backward, and then weights are updated using optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730cb9f-61cd-40d6-aaaf-1efa1d1a25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    # this sets the model mode - (i.e. layers like dropout, batchnorm etc behave differently during training compared to testing)\n",
    "    # note that this function was not defined explicitly in CNN, but because CNN is a type of nn.Module, it inherits some functions\n",
    "    # from the more general nn class.\n",
    "    cnn.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "       \n",
    "            b_x = images\n",
    "            b_y = labels\n",
    "            \n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "            pass     \n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58fafb8-f75e-4782-8a25-3ff8ab35a59a",
   "metadata": {},
   "source": [
    "After you have run everything above, call the training function below.\n",
    "\n",
    "**if you are running this on elab, note that it will take a *long* time to run. You may get faster results by running this on a local machine (it takese about 10 seconds on my laptop...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad724a7-f289-46a0-9b08-e33305d208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10a4e0-2545-4c8b-9e45-663f449576b3",
   "metadata": {},
   "source": [
    "In a standard gradient descent, you would expect the training loss to go down consistently. However, we are estimating the gradient each time on a subset of the data. This means that training loss can go up as well as down - however, over enough time, it can be shown that it will converge. One of the big advantages is that SGD requires a lot less memory, as we deal with only a small number of training examples at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4363c02-178f-4aa0-a7d0-27dc3587acbf",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "\n",
    "Now that the model has trained, let's see how well it does...\n",
    "\n",
    "Note that the CNN model does not have a sigmoid (or softmax) activation layer - this means that the outputs are not bound between 0 and 1. If we wanted the *probability* of each class, we would need to add a softmax layer to the output. In this case, we just care about the most likely class, so we can just select the index that corresponds to the highest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73a1ab-3d70-4ad7-81f6-9f87f7cfd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            test_output, last_layer = cnn(images) ## this runs the trained cnn. Note from the cnn class, test_output gives the model output (10 x 1 values), and last_layer gives the inputs into the last layer\n",
    "            ## torch.max finds the highest value of test_output. the [0] array element returns the maximum value, the [1] element\n",
    "            ## gives the index of that element. squeeze reshapes the data from a nx1 array into a list\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze() \n",
    "            #print(pred_y)\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        pass\n",
    "        print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    \n",
    "    pass\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7318035-f76e-4426-b421-93d8ccb8614e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
